{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea3a041a-b0d7-408b-b6d3-7b4828b618db",
   "metadata": {},
   "source": [
    "1.Precision:\n",
    "\n",
    "Definition: Precision is the ratio of correctly predicted positive observations to the total predicted positives.\n",
    "Formula:\n",
    "Precision\n",
    "=\n",
    "𝑇\n",
    "𝑃\n",
    "𝑇\n",
    "𝑃\n",
    "+\n",
    "𝐹\n",
    "𝑃\n",
    "Precision= \n",
    "TP+FP\n",
    "TP\n",
    "​\n",
    " \n",
    "Interpretation: High precision indicates a low false positive rate, meaning the model is good at predicting the positive class correctly when it does predict a positive.\n",
    "Recall:\n",
    "\n",
    "Definition: Recall (also known as Sensitivity or True Positive Rate) is the ratio of correctly predicted positive observations to all actual positives.\n",
    "Formula:\n",
    "Recall\n",
    "=\n",
    "𝑇\n",
    "𝑃\n",
    "𝑇\n",
    "𝑃\n",
    "+\n",
    "𝐹\n",
    "𝑁\n",
    "Recall= \n",
    "TP+FN\n",
    "TP\n",
    "​\n",
    " \n",
    "Interpretation: High recall indicates a low false negative rate, meaning the model is good at identifying all actual positives.\n",
    "Contextual Importance:\n",
    "\n",
    "Precision: Important in scenarios where false positives are costly (e.g., spam detection).\n",
    "Recall: Important in scenarios where false negatives are costly (e.g., cancer detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7051bdc-5c7a-46e3-98c6-cf7c32cb94b4",
   "metadata": {},
   "source": [
    "2.F1 Score:\n",
    "\n",
    "Definition: The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.\n",
    "Formula:\n",
    "F1 Score\n",
    "=\n",
    "2\n",
    "×\n",
    "Precision\n",
    "×\n",
    "Recall\n",
    "Precision\n",
    "+\n",
    "Recall\n",
    "F1 Score=2× \n",
    "Precision+Recall\n",
    "Precision×Recall\n",
    "​\n",
    " \n",
    "Interpretation: The F1 score gives a balance between precision and recall, and is especially useful when the classes are imbalanced.\n",
    "Differences:\n",
    "\n",
    "Precision: Measures the accuracy of positive predictions.\n",
    "Recall: Measures the ability to find all positive instances.\n",
    "F1 Score: Combines both precision and recall into a single metric to balance their trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f4b935-cd1b-4b70-89b2-61be5d091b4e",
   "metadata": {},
   "source": [
    "3.ROC (Receiver Operating Characteristic) Curve:\n",
    "\n",
    "Definition: A graphical plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied.\n",
    "Axes:\n",
    "X-axis: False Positive Rate (FPR) = \n",
    "𝐹\n",
    "𝑃\n",
    "𝐹\n",
    "𝑃\n",
    "+\n",
    "𝑇\n",
    "𝑁\n",
    "FP+TN\n",
    "FP\n",
    "​\n",
    " \n",
    "Y-axis: True Positive Rate (TPR) = \n",
    "𝑇\n",
    "𝑃\n",
    "𝑇\n",
    "𝑃\n",
    "+\n",
    "𝐹\n",
    "𝑁\n",
    "TP+FN\n",
    "TP\n",
    "​\n",
    " \n",
    "AUC (Area Under the Curve):\n",
    "\n",
    "Definition: The area under the ROC curve, which provides a single scalar value to summarize the model's performance.\n",
    "Interpretation:\n",
    "AUC = 1: Perfect model.\n",
    "AUC = 0.5: No better than random guessing.\n",
    "AUC < 0.5: Worse than random guessing.\n",
    "Usage:\n",
    "\n",
    "Model Evaluation: AUC-ROC is used to evaluate and compare the performance of classifiers, especially when dealing with imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edce14ea-50c5-4f60-9727-899920e16334",
   "metadata": {},
   "source": [
    "4.Choosing the Best Metric:\n",
    "\n",
    "Consider the Problem Context:\n",
    "Imbalance: For imbalanced datasets, metrics like precision, recall, F1 score, and AUC-ROC are more informative than accuracy.\n",
    "Cost of Errors: Consider the cost of false positives vs. false negatives. For instance, in medical diagnostics, recall might be more critical than precision.\n",
    "Evaluation Goals:\n",
    "Precision vs. Recall Trade-off: Use the F1 score for a balance, or choose based on whether precision or recall is more important.\n",
    "Threshold Sensitivity: Use AUC-ROC to evaluate the model's performance across all classification thresholds.\n",
    "Examples:\n",
    "\n",
    "Spam Detection: Precision to minimize false positives (legitimate emails marked as spam).\n",
    "Fraud Detection: Recall to catch as many fraudulent transactions as possible, even at the expense of more false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd74a42-065d-499c-868b-e348f19985ba",
   "metadata": {},
   "source": [
    "5.Multiclass Classification:\n",
    "\n",
    "Definition: Involves classifying instances into one of three or more classes.\n",
    "Example: Classifying images into categories like cats, dogs, and birds.\n",
    "Differences from Binary Classification:\n",
    "\n",
    "Classes: Binary classification deals with two classes (positive and negative), while multiclass deals with more than two.\n",
    "Approaches: Requires strategies to handle multiple classes, such as one-vs-rest (OvR) or one-vs-one (OvO) approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab26e9e-8d89-4651-bb07-8c2956445c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "6."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
